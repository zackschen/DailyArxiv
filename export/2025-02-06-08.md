# Continual Learning
## Optimal Task Order for Continual Learning of Multiple Tasks
- **Url**: http://arxiv.org/abs/2502.03350v1
- **Authors**: ['Ziyan Li', 'Naoki Hiratani']
- **Abstrat**: Continual learning of multiple tasks remains a major challenge for neural networks. Here, we investigate how task order influences continual learning and propose a strategy for optimizing it. Leveraging a linear teacher-student model with latent factors, we derive an analytical expression relating task similarity and ordering to learning performance. Our analysis reveals two principles that hold under a wide parameter range: (1) tasks should be arranged from the least representative to the most typical, and (2) adjacent tasks should be dissimilar. We validate these rules on both synthetic data and real-world image classification datasets (Fashion-MNIST, CIFAR-10, CIFAR-100), demonstrating consistent performance improvements in both multilayer perceptrons and convolutional neural networks. Our work thus presents a generalizable framework for task-order optimization in task-incremental continual learning.


**Summary**: 

- (1): 本文研究的背景是深度神经网络在多任务连续学习中的挑战，尤其是"灾难性遗忘"现象。

- (2): 过去的方法主要研究了任务顺序对学习性能的影响，但缺乏对如何优化任务顺序的明确理解。本文提出的策略通过线性教师-学生模型引入了任务相似性与顺序与学习性能的关系，解决了这一问题。提出的方法具有理论动机，强调了从不具代表性的任务到典型任务的学习顺序，并确保相邻任务的相似性较低。

- (3): 论文的贡献在于提供了任务顺序优化的普遍框架，确定了两项规则：1) 从最不具代表性的任务学习到最典型的任务，2) 相邻任务应具有较大相似性差异，从而提升持续学习的性能。

- (4): 本文的方法论包括推导出与任务相似性相关的平均误差的分析表达式，利用线性扰动分析探讨任务顺序对学习性能的影响，最终提出了"周边到核心规则"和"最大路径规则"。

- (5): 论文在Fashion-MNIST、CIFAR-10和CIFAR-100数据集上进行了评估，证明在多层感知器和卷积神经网络上均实现了一致的性能提升，符合提出的目标。


# Incremental Learning
## SPARC: Subspace-Aware Prompt Adaptation for Robust Continual Learning in LLMs
- **Url**: http://arxiv.org/abs/2502.02909v1
- **Authors**: ['Dinithi Jayasuriya', 'Sina Tayebati', 'Davide Ettori', 'Ranganath Krishnan', 'Amit Ranjan Trivedi']
- **Abstrat**: We propose SPARC, a lightweight continual learning framework for large language models (LLMs) that enables efficient task adaptation through prompt tuning in a lower-dimensional space. By leveraging principal component analysis (PCA), we identify a compact subspace of the training data. Optimizing prompts in this lower-dimensional space enhances training efficiency, as it focuses updates on the most relevant features while reducing computational overhead. Furthermore, since the model's internal structure remains unaltered, the extensive knowledge gained from pretraining is fully preserved, ensuring that previously learned information is not compromised during adaptation. Our method achieves high knowledge retention in both task-incremental and domain-incremental continual learning setups while fine-tuning only 0.04% of the model's parameters. Additionally, by integrating LoRA, we enhance adaptability to computational constraints, allowing for a tradeoff between accuracy and training cost. Experiments on the SuperGLUE benchmark demonstrate that our PCA-based prompt tuning combined with LoRA maintains full knowledge retention while improving accuracy, utilizing only 1% of the model's parameters. These results establish our approach as a scalable and resource-efficient solution for continual learning in LLMs.


**Summary**: 

- (1): 本文研究背景是大型语言模型（LLMs）在自然语言处理中的广泛应用，但在动态环境中需要有效的逐步学习能力，以避免灾难性遗忘。

- (2): 过去的方法包括重播法、正则化方法（如Elastic Weight Consolidation和Synaptic Intelligence）和参数效率微调（PEFT）方法。它们的问题在于，重播法带来了内存开销，正则化方法在高维参数空间中效果不佳，PEFT方法则常常需要架构修改。所提方法SPARC通过降低维度的提示调优解决了这些问题，避免了重复和灾难性遗忘，具有很好的动机。

- (3): 本文的贡献在于提出了SPARC，一个轻量级的逐步学习框架，通过主成分分析（PCA）优化提示在低维空间中进行任务适应，显著提高了训练效率并保持了知识保持。

- (4): 本文提出的方法是基于主成分分析（PCA）进行低维空间的提示调优，仅微调0.04%模型参数，并结合LoRA以适应计算约束，实现了高效的任务特定适应。

- (5): 在SuperGLUE基准测试上，所提方法仅使用1%模型参数，达到了良好的准确性并维持了完整的知识保持，证明了其性能支持逐步学习的目标。

