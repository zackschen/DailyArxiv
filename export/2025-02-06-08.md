# Continual Learning
## Optimal Task Order for Continual Learning of Multiple Tasks
- **Url**: http://arxiv.org/abs/2502.03350v1
- **Authors**: ['Ziyan Li', 'Naoki Hiratani']
- **Abstrat**: Continual learning of multiple tasks remains a major challenge for neural networks. Here, we investigate how task order influences continual learning and propose a strategy for optimizing it. Leveraging a linear teacher-student model with latent factors, we derive an analytical expression relating task similarity and ordering to learning performance. Our analysis reveals two principles that hold under a wide parameter range: (1) tasks should be arranged from the least representative to the most typical, and (2) adjacent tasks should be dissimilar. We validate these rules on both synthetic data and real-world image classification datasets (Fashion-MNIST, CIFAR-10, CIFAR-100), demonstrating consistent performance improvements in both multilayer perceptrons and convolutional neural networks. Our work thus presents a generalizable framework for task-order optimization in task-incremental continual learning.


**Summary**: 

- (1): 本文研究的背景是深度神经网络在多任务连续学习中的挑战，尤其是"灾难性遗忘"现象。

- (2): 过去的方法主要研究了任务顺序对学习性能的影响，但缺乏对如何优化任务顺序的明确理解。本文提出的策略通过线性教师-学生模型引入了任务相似性与顺序与学习性能的关系，解决了这一问题。提出的方法具有理论动机，强调了从不具代表性的任务到典型任务的学习顺序，并确保相邻任务的相似性较低。

- (3): 论文的贡献在于提供了任务顺序优化的普遍框架，确定了两项规则：1) 从最不具代表性的任务学习到最典型的任务，2) 相邻任务应具有较大相似性差异，从而提升持续学习的性能。

- (4): 本文的方法论包括推导出与任务相似性相关的平均误差的分析表达式，利用线性扰动分析探讨任务顺序对学习性能的影响，最终提出了"周边到核心规则"和"最大路径规则"。

- (5): 论文在Fashion-MNIST、CIFAR-10和CIFAR-100数据集上进行了评估，证明在多层感知器和卷积神经网络上均实现了一致的性能提升，符合提出的目标。


# Incremental Learning
## SPARC: Subspace-Aware Prompt Adaptation for Robust Continual Learning in LLMs
- **Url**: http://arxiv.org/abs/2502.02909v1
- **Authors**: ['Dinithi Jayasuriya', 'Sina Tayebati', 'Davide Ettori', 'Ranganath Krishnan', 'Amit Ranjan Trivedi']
- **Abstrat**: We propose SPARC, a lightweight continual learning framework for large language models (LLMs) that enables efficient task adaptation through prompt tuning in a lower-dimensional space. By leveraging principal component analysis (PCA), we identify a compact subspace of the training data. Optimizing prompts in this lower-dimensional space enhances training efficiency, as it focuses updates on the most relevant features while reducing computational overhead. Furthermore, since the model's internal structure remains unaltered, the extensive knowledge gained from pretraining is fully preserved, ensuring that previously learned information is not compromised during adaptation. Our method achieves high knowledge retention in both task-incremental and domain-incremental continual learning setups while fine-tuning only 0.04% of the model's parameters. Additionally, by integrating LoRA, we enhance adaptability to computational constraints, allowing for a tradeoff between accuracy and training cost. Experiments on the SuperGLUE benchmark demonstrate that our PCA-based prompt tuning combined with LoRA maintains full knowledge retention while improving accuracy, utilizing only 1% of the model's parameters. These results establish our approach as a scalable and resource-efficient solution for continual learning in LLMs.


**Summary**: 

- (1): 本文研究背景是大型语言模型（LLMs）在自然语言处理中的广泛应用，但在动态环境中需要有效的逐步学习能力，以避免灾难性遗忘。

- (2): 过去的方法包括重播法、正则化方法（如Elastic Weight Consolidation和Synaptic Intelligence）和参数效率微调（PEFT）方法。它们的问题在于，重播法带来了内存开销，正则化方法在高维参数空间中效果不佳，PEFT方法则常常需要架构修改。所提方法SPARC通过降低维度的提示调优解决了这些问题，避免了重复和灾难性遗忘，具有很好的动机。

- (3): 本文的贡献在于提出了SPARC，一个轻量级的逐步学习框架，通过主成分分析（PCA）优化提示在低维空间中进行任务适应，显著提高了训练效率并保持了知识保持。

- (4): 本文提出的方法是基于主成分分析（PCA）进行低维空间的提示调优，仅微调0.04%模型参数，并结合LoRA以适应计算约束，实现了高效的任务特定适应。

- (5): 在SuperGLUE基准测试上，所提方法仅使用1%模型参数，达到了良好的准确性并维持了完整的知识保持，证明了其性能支持逐步学习的目标。

# Continual Learning
## Optimal Task Order for Continual Learning of Multiple Tasks
- **Url**: http://arxiv.org/abs/2502.03350v1
- **Authors**: ['Ziyan Li', 'Naoki Hiratani']
- **Abstrat**: Continual learning of multiple tasks remains a major challenge for neural networks. Here, we investigate how task order influences continual learning and propose a strategy for optimizing it. Leveraging a linear teacher-student model with latent factors, we derive an analytical expression relating task similarity and ordering to learning performance. Our analysis reveals two principles that hold under a wide parameter range: (1) tasks should be arranged from the least representative to the most typical, and (2) adjacent tasks should be dissimilar. We validate these rules on both synthetic data and real-world image classification datasets (Fashion-MNIST, CIFAR-10, CIFAR-100), demonstrating consistent performance improvements in both multilayer perceptrons and convolutional neural networks. Our work thus presents a generalizable framework for task-order optimization in task-incremental continual learning.


**Summary**: 

- (1): 本文研究的背景是神经网络在持续学习多任务时面临的挑战，尤其是如何避免灾难性遗忘并促进知识转移。

- (2): 过去的方法主要探讨任务顺序对持续学习的影响，存在缺乏清晰理解任务排序如何影响学习性能的问题。与现有方法相比，本文提出了一种优化任务顺序的策略，通过分析任务相似性，从最不具代表性的任务到最具代表性的任务进行排列，并确保相邻任务间的差异性。该方法针对现有问题提供了解释，并具有良好的动机。

- (3): 本文的贡献在于提供了一个可推广的任务顺序优化框架，揭示了任务顺序依赖的两个原则，丰富了持续学习领域的理论基础。

- (4): 本文提出的研究方法基于带有潜在因子的线性教师-学生模型，推导出任务相似性与学习性能之间的解析关系，并通过实验验证了所提出的任务排序原则。

- (5): 本文在 Fashion-MNIST、CIFAR-10 和 CIFAR-100 数据集上进行的持续图像分类任务中实现了显著的性能提升，结果展示了所提出的方法达到了其目标。


# Incremental Learning
## SPARC: Subspace-Aware Prompt Adaptation for Robust Continual Learning in LLMs
- **Url**: http://arxiv.org/abs/2502.02909v1
- **Authors**: ['Dinithi Jayasuriya', 'Sina Tayebati', 'Davide Ettori', 'Ranganath Krishnan', 'Amit Ranjan Trivedi']
- **Abstrat**: We propose SPARC, a lightweight continual learning framework for large language models (LLMs) that enables efficient task adaptation through prompt tuning in a lower-dimensional space. By leveraging principal component analysis (PCA), we identify a compact subspace of the training data. Optimizing prompts in this lower-dimensional space enhances training efficiency, as it focuses updates on the most relevant features while reducing computational overhead. Furthermore, since the model's internal structure remains unaltered, the extensive knowledge gained from pretraining is fully preserved, ensuring that previously learned information is not compromised during adaptation. Our method achieves high knowledge retention in both task-incremental and domain-incremental continual learning setups while fine-tuning only 0.04% of the model's parameters. Additionally, by integrating LoRA, we enhance adaptability to computational constraints, allowing for a tradeoff between accuracy and training cost. Experiments on the SuperGLUE benchmark demonstrate that our PCA-based prompt tuning combined with LoRA maintains full knowledge retention while improving accuracy, utilizing only 1% of the model's parameters. These results establish our approach as a scalable and resource-efficient solution for continual learning in LLMs.


**Summary**: 

- (1): 本文的研究背景是大型语言模型（LLMs）在自然语言处理中的广泛应用，以及其在动态环境中持续学习和适应新任务的必要性。

- (2): 过去的方法包括重放-based、正则化-based 和参数高效微调（PEFT）等方法，存在内存开销大、效率低，以及容易导致灾难性遗忘等问题。本文提出的SPARC方法通过利用主成分分析（PCA）在低维空间中进行提示调优，相较于现有方法，能有效提高计算效率并保持知识保留，不改变模型的内部结构，具有良好的动机。

- (3): 该论文的贡献在于提出了一种新的轻量级连续学习框架SPARC，能够通过低维空间中的提示调优实现高效的任务适应，并为LLMs提供了一种可扩展且资源高效的解决方案。

- (4): 本文提出的方法是在低维空间中进行提示调优，通过PCA识别训练数据的紧凑子空间，仅微调0.04%的模型参数，并结合LoRA增强计算适应性。

- (5): 本文在SuperGLUE基准上进行实验，结果显示通过PCA基础的提示调优和LoRA，能够保持完整的知识保留并提升精度，仅使用1%的模型参数。这些性能结果支持了论文的目标。

