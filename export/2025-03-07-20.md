# Continual Learning
## Teach YOLO to Remember: A Self-Distillation Approach for Continual Object Detection
- **Url**: http://arxiv.org/abs/2503.04688v1
- **Authors**: ['Riccardo De Monte', 'Davide Dalle Pezze', 'Gian Antonio Susto']
- **Abstrat**: Real-time object detectors like YOLO achieve exceptional performance when trained on large datasets for multiple epochs. However, in real-world scenarios where data arrives incrementally, neural networks suffer from catastrophic forgetting, leading to a loss of previously learned knowledge. To address this, prior research has explored strategies for Class Incremental Learning (CIL) in Continual Learning for Object Detection (CLOD), with most approaches focusing on two-stage object detectors. However, existing work suggests that Learning without Forgetting (LwF) may be ineffective for one-stage anchor-free detectors like YOLO due to noisy regression outputs, which risk transferring corrupted knowledge. In this work, we introduce YOLO LwF, a self-distillation approach tailored for YOLO-based continual object detection. We demonstrate that when coupled with a replay memory, YOLO LwF significantly mitigates forgetting. Compared to previous approaches, it achieves state-of-the-art performance, improving mAP by +2.1% and +2.9% on the VOC and COCO benchmarks, respectively.


**Summary**: 

- (1): 本文的研究背景是实时目标检测在实时应用中的表现，尤其是在数据以增量形式到达的现实场景中，神经网络面临灾难性遗忘的问题。

- (2): 过去的方法主要集中在两阶段的目标检测器上，尤其是Class Incremental Learning (CIL)和Learning without Forgetting (LwF)。这些方法在一种单阶段的无锚检测器（如YOLO）上效果不佳，因其回归输出的噪声可能导致知识转移受损。提议的方法YOLO LwF通过自蒸馏方法专门针对YOLO进行设计，结合重放记忆显著减轻遗忘问题。

- (3): 论文的贡献在于提出了一种针对YOLO的自蒸馏方法YOLO LwF，能有效缓解灾难性遗忘，并在性能上超过现有方法，在VOC和COCO基准上分别提高了mAP 2.1%和2.9%。

- (4): 研究方法论包括引入YOLO LwF的自蒸馏机制，并结合重放记忆来保存和利用先前任务的知识，增强YOLO的连续学习能力。

- (5): 本文的方法在目标检测任务上取得了显著的性能，VOC和COCO基准上分别提高了mAP 2.1%和2.9%，支持了他们克服灾难性遗忘的目标。

