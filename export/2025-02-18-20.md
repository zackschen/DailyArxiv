# Continual Learning
## Metalearning Continual Learning Algorithms
- **Url**: http://arxiv.org/abs/2312.00276v3
- **Authors**: ['Kazuki Irie', 'Róbert Csordás', 'Jürgen Schmidhuber']
- **Abstrat**: General-purpose learning systems should improve themselves in open-ended fashion in ever-changing environments. Conventional learning algorithms for neural networks, however, suffer from catastrophic forgetting (CF), i.e., previously acquired skills are forgotten when a new task is learned. Instead of hand-crafting new algorithms for avoiding CF, we propose Automated Continual Learning (ACL) to train self-referential neural networks to metalearn their own in-context continual (meta)learning algorithms. ACL encodes continual learning (CL) desiderata -- good performance on both old and new tasks -- into its metalearning objectives. Our experiments demonstrate that ACL effectively resolves "in-context catastrophic forgetting," a problem that naive in-context learning algorithms suffer from; ACL-learned algorithms outperform both hand-crafted learning algorithms and popular meta-continual learning methods on the Split-MNIST benchmark in the replay-free setting, and enables continual learning of diverse tasks consisting of multiple standard image classification datasets. We also discuss the current limitations of in-context CL by comparing ACL with state-of-the-art CL methods that leverage pre-trained models. Overall, we bring several novel perspectives into the long-standing problem of CL.


**Summary**: 

- (1): 本文的研究背景为需要在不断变化的环境中自我改进的通用学习系统，而传统神经网络学习算法在学习新任务时会遭遇灾难性遗忘（CF）的问题。

- (2): 过去的方法主要关注手工设计避免CF的新算法，但通常这些方法在面对序列任务时表现不佳。本文提出的Automated Continual Learning (ACL) 方法通过训练自我参考的神经网络来元学习自身的持续学习算法，有效应对CF问题。

- (3): 本文的贡献在于提出了ACL，能够有效解决“上下文中的灾难性遗忘”问题，并在Replay-free设置下，ACL学习的算法在Split-MNIST基准上超越了手工设计的学习算法及流行的元持续学习方法。

- (4): 本文的研究方法是编码持续学习（CL）需求，通过元学习目标优化，形成自我学习算法的框架，使神经网络能够处理多种标准图像分类数据集。

- (5): 在Split-MNIST基准和多个标准图像分类任务中，ACL展示了优越的性能，支持了其在持续学习中的目标。

