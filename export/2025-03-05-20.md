# Continual Learning
## Beyond Cosine Decay: On the effectiveness of Infinite Learning Rate Schedule for Continual Pre-training
- **Url**: http://arxiv.org/abs/2503.02844v1
- **Authors**: ['Paul Janson', 'Vaibhav Singh', 'Paria Mehrbod', 'Adam Ibrahim', 'Irina Rish', 'Eugene Belilovsky', 'Benjamin Thérien']
- **Abstrat**: The ever-growing availability of unlabeled data presents both opportunities and challenges for training artificial intelligence systems. While self-supervised learning (SSL) has emerged as a powerful paradigm for extracting meaningful representations from vast amounts of unlabeled data, existing methods still struggle to adapt to the non-stationary, non-IID nature of real-world data streams without forgetting previously learned knowledge. Recent works have adopted a repeated cosine annealing schedule for large-scale continual pre-training; however, these schedules (1) inherently cause forgetting during the re-warming phase and (2) have not been systematically compared to existing continual SSL methods. In this work, we systematically compare the widely used cosine schedule with the recently proposed infinite learning rate schedule and empirically find the latter to be a more effective alternative. Our extensive empirical evaluation across diverse image and language datasets demonstrates that the infinite learning rate schedule consistently enhances continual pre-training performance compared to a repeated cosine decay without being restricted to a fixed iteration budget. For instance, in a small-scale MAE pre-training setup, it outperforms several strong baselines from the literature. We then scale up our experiments to larger MAE pre-training and autoregressive language model pre-training. Our results show that the infinite learning rate schedule remains effective at scale, surpassing repeated cosine decay for both MAE pre-training and zero-shot LM benchmarks.


**Summary**: 

- (1): 本文的研究背景是自监督学习（SSL）的兴起，它能够从大量未标记数据中提取有意义的表示，但现有方法在处理非平稳、不独立同分布（non-IID）的数据流时容易遗忘之前学到的知识。
  
- (2): 过去的方法主要采用重复的余弦衰减调度，但这导致在重新加热阶段不可避免地出现遗忘，并且尚未与现有的持续自监督学习方法进行系统比较。本文提出的无限学习率调度（infinite learning rate schedule）较之更为有效，能够在无需固定迭代预算的情况下提升持续预训练性能。该方法通过不断保持学习率，避免了衰减带来的遗忘问题，具有明确的动机。

- (3): 文章的贡献在于系统比较了余弦调度和无限学习率调度，并通过大量实验证明无限学习率调度在多样的图像和语言数据集上均有效提升了持续预训练的性能。

- (4): 本文提出的研究方法包括在小规模和大规模的预训练任务中应用无限学习率调度，对比传统的余弦调度，在多个实验中进行评估。

- (5): 本文在小规模 MAE 预训练和自回归语言模型预训练任务上取得了超越多种强基线的性能，表明所提方法能够有效支持其目标。

