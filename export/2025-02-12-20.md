# Continual Learning
## An Efficient Rehearsal Scheme for Catastrophic Forgetting Mitigation during Multi-stage Fine-tuning
- **Url**: http://arxiv.org/abs/2402.08096v3
- **Authors**: ['Andrew Bai', 'Chih-Kuan Yeh', 'Cho-Jui Hsieh', 'Ankur Taly']
- **Abstrat**: Incrementally fine-tuning foundational models on new tasks or domains is now the de facto approach in NLP. A known pitfall of this approach is the \emph{catastrophic forgetting} of prior knowledge that happens during fine-tuning. A common approach to alleviate such forgetting is to rehearse samples from prior tasks during fine-tuning. Several existing works assume a fixed memory buffer to store prior task examples, while relying on inferences (forward passes) with the model at hand for choosing examples for rehearsal from the buffer. However, given the increasing computational cost of model inference, and decreasing cost of data storage, we focus on the setting to rehearse samples with a fixed computational budget instead of a fixed memory budget. We propose a sampling scheme, \texttt{\bf mix-cd}, that prioritizes rehearsal of ``collateral damage'' samples, which are samples predicted correctly by the prior model but forgotten by the incrementally tuned one. The crux of our scheme is a procedure to efficiently estimate the density of collateral damage samples without incurring additional model inferences. Our approach is computationally efficient, easy to implement, and outperforms several leading continual learning methods in compute-constrained settings. All the code will be publicly available at https://github.com/jybai/mix-cd-rehearsal.


**Summary**: 

- (1): 本文研究的背景是Incrementally fine-tuning基础模型在自然语言处理（NLP）中的应用，而这种方法的一个已知缺陷是导致先前知识的灾难性遗忘。

- (2): 过去的方法通常依赖一个固定的记忆缓冲区，从中存储先前任务的样本，并利用模型推理选择进行重 rehearse 的示例。这些方法存在存储成本和高推理计算成本的问题。本文提出了一种不同的方法，即mix-cd采样方案，优先回 rehearse 被之前模型正确预测但被增量调优遗忘的“附带损伤”样本。该方法通过高效估计样本密度来解决所有问题，而不需要额外的模型推理，切实优化了计算效率。

- (3): 本文的贡献在于提出了一种在计算预算固定的情况下有效的重 rehearse 方案mix-cd，能够有效减少灾难性遗忘，同时具备易于实现的特点，并在计算受限的环境中超过了多种领先的持续学习方法。

- (4): 本文研究的方法论包括一种采样策略mix-cd，该策略在固定的计算预算下，优先选择被之前模型正确预测但在增量调优后被遗忘的样本，并通过估计其密度来实现这一目标。

- (5): 本文的方法在多阶段的微调任务上表现出色，超越了多种现有的加速学习方法，且其性能能够支持减轻灾难性遗忘的目标。

